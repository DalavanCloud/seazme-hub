* Objective

Provide easy to use and extensible API for uploading information to datahub from various sources.

datahub is aware of the source data type (e.g. Confluence, GitHub, etc), however after a simple validation, it stores the data in almost a raw format, while maintaining normalized metada about the sources. It is up to internal modules to process it further, e.g. to upload it to ES or perform certain analitics.

In most cases, it is not possible to scan entire source at once and push the content in a single POST. Therefore, SQL like transaction model is followed where we PUT begin/insert/commit/rollbacl statements as resources for datahub to perform.

The datahub design follows "never delete" and "never update" http://www.datomic.com like paradim. Instead of meticulusly maintaining the "current" info, data is stored as a chain of "intake" events and transactions and all subsequent processing will be taken from there. That enables flexibility and openess for future changes as well.

* Use cases

** API key provisioning

This action is to be performed by a person from a browser pointing at DataHub service itself, e.g. withing Swagger reviewer and tester. At that point the user is already SSO logged in and autheticated.

   [[/docs/create-user.png]]

** Full Scan

   [[/docs/full-scan.png]]

** Incremental Scan

   [[/docs/incremental-scan.png]]

* Open Questions:

- how we approach it gathering data from GitHub?
- will gap detection be supported?

